{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Social Media Post Recommendation Using Reddit Dataset**                       \n",
        "In this project, we developed a Reddit post recommendation system using semantic search and fine-tuned lightweight language models. We used SBERT embeddings and FAISS indexing to retrieve top-k similar posts based on user input. To enhance contextual generation, we fine-tuned three different LLMs ‚Äî DeepSeek-7B, TinyLlama-1.1B, and Mistral-7B-Instruct ‚Äî using LoRA for efficient adaptation. These models generate human-like post recommendations that reflect community-driven responses. We further evaluated the quality of generated outputs using BLEU, ROUGE, and BERTScore metrics to assess relevance, fluency, and semantic similarity. This system enables context-aware content recommendation using scalable and resource-efficient architectures."
      ],
      "metadata": {
        "id": "LF0LMQ68LS4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Libraries"
      ],
      "metadata": {
        "id": "khXQtWYELg71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "1DAjUEZDKFBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading Dataset From CSV File"
      ],
      "metadata": {
        "id": "XWEpQqpvLmmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "df = pd.read_csv(\"G2FinalDatasetReddit.csv\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "xlf2wjQYKFHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing and Cleaning Dataset"
      ],
      "metadata": {
        "id": "gcvd0OjULskK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'<.*?>+', '', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['clean_body'] = df['body'].apply(clean_text)\n",
        "df['clean_body']"
      ],
      "metadata": {
        "id": "HtUMKTAiKFN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['clean_body'].str.len() >= 10].reset_index(drop=True)\n",
        "df"
      ],
      "metadata": {
        "id": "Pws8M6FqKFTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving CLeaned Dataset Into New CSV File"
      ],
      "metadata": {
        "id": "qh5e-LPlL3sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"Cleaned_Reddit_Comments.csv\", index=False)"
      ],
      "metadata": {
        "id": "XbiSwe5XKFZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O9cLVpI8KFem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing senetnce-Transformers"
      ],
      "metadata": {
        "id": "mQlgaNgQMAdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "id": "PysZZ9LTKFgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Of Dataset"
      ],
      "metadata": {
        "id": "xLEXzyMJMG9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Load model and encode\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = sbert_model.encode(df['clean_body'].tolist(), show_progress_bar=True)\n",
        "np.save(\"sbert_embeddings.npy\", embeddings)\n"
      ],
      "metadata": {
        "id": "sy9BuG6HKFk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate\n",
        "!pip install safetensors\n"
      ],
      "metadata": {
        "id": "pcf0DNcGKFmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading Deepseek Model For Recommending Posts**"
      ],
      "metadata": {
        "id": "uJYJHV3mMPEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-llm-7b-base\", trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"deepseek-ai/deepseek-llm-7b-base\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    offload_folder=\"offload\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "2BcV9nORKFtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommending Posts By Prompt Base Finetuning Using Deepseek Model"
      ],
      "metadata": {
        "id": "ZAgFXwagMZRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_with_deepseek(query, k=5):\n",
        "    top_posts = get_top_k_similar_posts(query, k)\n",
        "    prompt = f\"\"\"You're a Reddit post recommender. Given a user's post: \"{query}\", and the following similar posts:\\n\\n\"\"\"\n",
        "    for i, post in enumerate(top_posts):\n",
        "        prompt += f\"{i+1}. {post}\\n\"\n",
        "    prompt += \"\\nGenerate a recommendation summary or suggest the most relevant content for the user.\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=150, do_sample=True, top_p=0.9, temperature=0.7)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "bMkJKPunKF0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I'm feeling stressed about exams, what should I do?\"\n",
        "print(recommend_with_deepseek(query, k=5))\n"
      ],
      "metadata": {
        "id": "F0oThTshKF5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing Lora Finetuning of Deepseek Model and Saving Trained Model"
      ],
      "metadata": {
        "id": "r95mja-iMmCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "id": "IfGmBt8sKGAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    TaskType,\n",
        ")\n",
        "\n",
        "# === STEP 1: Load cleaned dataset and SBERT embeddings ===\n",
        "df = pd.read_csv(\"Cleaned_Reddit_Comments.csv\")\n",
        "embeddings = np.load(\"sbert_embeddings.npy\").astype(\"float32\")\n",
        "\n",
        "assert len(df) == len(embeddings), \"Mismatch between embeddings and dataset!\"\n",
        "\n",
        "# === STEP 2: Build FAISS index for similarity search ===\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "# === STEP 3: Construct prompt-completion pairs using top-k similar posts ===\n",
        "k = 5\n",
        "records = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    query_vector = embeddings[i].reshape(1, -1)\n",
        "    _, top_k = index.search(query_vector, k + 1)  # +1 because it includes the post itself\n",
        "    top_k = [idx for idx in top_k[0] if idx != i][:k]\n",
        "\n",
        "    similar_posts = df.iloc[top_k]['clean_body'].tolist()\n",
        "    prompt = (\n",
        "        f\"User post from subreddit r/{df.iloc[i]['subreddit']}:\\n\\n\"\n",
        "        f\"{df.iloc[i]['clean_body']}\\n\\n\"\n",
        "        f\"Similar posts:\\n\" +\n",
        "        \"\\n\".join([f\"{j+1}. {p}\" for j, p in enumerate(similar_posts)]) +\n",
        "        \"\\n\\n###\"\n",
        "    )\n",
        "\n",
        "    completion = \" Recommended content: [Insert relevant recommendation here]\"\n",
        "    records.append({\"prompt\": prompt, \"completion\": completion})\n",
        "\n",
        "# Save to JSONL\n",
        "jsonl_path = \"reddit_lora_dataset.jsonl\"\n",
        "with open(jsonl_path, \"w\") as f:\n",
        "    for r in records:\n",
        "        f.write(json.dumps(r) + \"\\n\")\n",
        "\n",
        "print(f\"‚úÖ Dataset prepared and saved to {jsonl_path}\")\n",
        "\n",
        "# === STEP 4: Load model & tokenizer ===\n",
        "model_name = \"deepseek-ai/deepseek-llm-7b-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    offload_folder=\"offload\"\n",
        ")\n",
        "\n",
        "# === STEP 5: Apply LoRA via PEFT ===\n",
        "base_model = prepare_model_for_kbit_training(base_model)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# === STEP 6: Load and tokenize dataset ===\n",
        "dataset = load_dataset(\"json\", data_files=jsonl_path)\n",
        "\n",
        "def format_example(e):\n",
        "    return f\"{e['prompt']}{e['completion']}\"\n",
        "\n",
        "def tokenize_example(e):\n",
        "    return tokenizer(format_example(e), truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_example, remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "# === STEP 7: Training setup ===\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./deepseek-lora-reddit\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# === STEP 8: Finetune the model ===\n",
        "print(\"üöÄ Starting finetuning...\")\n",
        "trainer.train()\n",
        "\n",
        "# === STEP 9: Save the finetuned model ===\n",
        "model.save_pretrained(\"deepseek-lora-reddit\")\n",
        "tokenizer.save_pretrained(\"deepseek-lora-reddit\")\n",
        "print(\"‚úÖ Finetuned model saved to ./deepseek-lora-reddit\")\n"
      ],
      "metadata": {
        "id": "-qMx8W4xKGCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GGmfPinjKGH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommending or Inferencing Posts Using Trained Deepseek Model by using Lora Technique"
      ],
      "metadata": {
        "id": "8K4M11W9MzUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# === Load preprocessed Reddit data and embeddings ===\n",
        "df = pd.read_csv(\"Cleaned_Reddit_Comments.csv\")\n",
        "embeddings = np.load(\"sbert_embeddings.npy\").astype(\"float32\")\n",
        "\n",
        "# === Load FAISS index ===\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "# === Load SBERT model to encode query ===\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# === Load base + LoRA-finetuned DeepSeek model ===\n",
        "base = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-llm-7b-base\", device_map=\"auto\", torch_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-lora-reddit\", trust_remote_code=True)\n",
        "model = PeftModel.from_pretrained(base, \"deepseek-lora-reddit\")\n",
        "\n",
        "# === Text generation pipeline ===\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# === Inference Function ===\n",
        "def recommend_reddit_post(user_input, k=5):\n",
        "    # Step 1: Embed user query\n",
        "    query_embed = sbert_model.encode([user_input]).astype(\"float32\")\n",
        "\n",
        "    # Step 2: Get top-k similar posts from FAISS\n",
        "    _, top_k_indices = index.search(query_embed, k)\n",
        "    similar_posts = df.iloc[top_k_indices[0]][\"clean_body\"].tolist()\n",
        "\n",
        "    # Step 3: Construct prompt for generation\n",
        "    prompt = (\n",
        "        f\"User post:\\n\\n\"\n",
        "        f\"{user_input}\\n\\n\"\n",
        "        f\"Similar posts:\\n\" +\n",
        "        \"\\n\".join([f\"{i+1}. {post}\" for i, post in enumerate(similar_posts)]) +\n",
        "        \"\\n\\n###\\n\"\n",
        "    )\n",
        "\n",
        "    # Step 4: Generate output\n",
        "    output = pipe(prompt, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.7)\n",
        "    return output[0]['generated_text']\n",
        "\n",
        "# === Example Usage ===\n",
        "user_input = \"My cat climbs on my laptop every time I open it.\"\n",
        "result = recommend_reddit_post(user_input)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "ckXVQdTlKGJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oRDLeZ8yKGOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Another LLM Named as TinyLlama-1.1B For Recommending Posts"
      ],
      "metadata": {
        "id": "BLxLzNdyNHVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "\n",
        "# === Load Tokenizer and Base Model ===\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# === Prepare for LoRA Training ===\n",
        "base_model = prepare_model_for_kbit_training(base_model)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "# === Load JSONL Dataset ===\n",
        "dataset = load_dataset(\"json\", data_files=\"reddit_lora_dataset.jsonl\")\n",
        "\n",
        "def format_prompt(e):\n",
        "    return f\"{e['prompt']}{e['completion']}\"\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(format_prompt(example), truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "# === Training Setup ===\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./tinyllama-lora-reddit\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# === Save ===\n",
        "model.save_pretrained(\"tinyllama-lora-reddit\")\n",
        "tokenizer.save_pretrained(\"tinyllama-lora-reddit\")\n"
      ],
      "metadata": {
        "id": "GhvjmcEIKGQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "neeuD_QFKGVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inferencing Based On TinyLlama Model"
      ],
      "metadata": {
        "id": "nTlTf8i6NWeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import numpy as np\n",
        "import faiss\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load\n",
        "df = pd.read_csv(\"Cleaned_Reddit_Comments.csv\")\n",
        "embeddings = np.load(\"sbert_embeddings.npy\").astype(\"float32\")\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "base = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", device_map=\"auto\", torch_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"tinyllama-lora-reddit\")\n",
        "model = PeftModel.from_pretrained(base, \"tinyllama-lora-reddit\")\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def recommend(user_input, k=5):\n",
        "    query = sbert.encode([user_input]).astype(\"float32\")\n",
        "    _, top_k = index.search(query, k)\n",
        "    similar = df.iloc[top_k[0]][\"clean_body\"].tolist()\n",
        "    prompt = f\"User post:\\n\\n{user_input}\\n\\nSimilar posts:\\n\" + \"\\n\".join([f\"{i+1}. {p}\" for i, p in enumerate(similar)]) + \"\\n\\n###\\n\"\n",
        "    return pipe(prompt, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.7)[0]['generated_text']\n",
        "\n",
        "print(recommend(\"Why do dogs tilt their heads when you talk to them?\"))\n"
      ],
      "metadata": {
        "id": "UQSDtcTqKGXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Third Model For Recommending Posts Named as Mistral-7B-Instruct"
      ],
      "metadata": {
        "id": "aR68tiSSN0Hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers peft accelerate sentence-transformers faiss-cpu\n"
      ],
      "metadata": {
        "id": "2x3WZI3DKGeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "base_model = prepare_model_for_kbit_training(base_model)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=\"reddit_lora_dataset.jsonl\")\n",
        "tokenized_dataset = dataset.map(tokenize, remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mistral-lora-reddit\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"mistral-lora-reddit\")\n",
        "tokenizer.save_pretrained(\"mistral-lora-reddit\")\n"
      ],
      "metadata": {
        "id": "pPI4Q4mWKGjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inferencing Based on Third Model"
      ],
      "metadata": {
        "id": "wy8HxHGhN465"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from peft import PeftModel\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# === Load Reddit Post Data and Embeddings ===\n",
        "df = pd.read_csv(\"Cleaned_Reddit_Comments.csv\")  # Must have a 'clean_body' column\n",
        "embeddings = np.load(\"sbert_embeddings.npy\").astype(\"float32\")\n",
        "\n",
        "# === Create FAISS Index for Fast Similarity Search ===\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "# === Load SBERT for encoding user query ===\n",
        "sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# === Load Mistral Base and LoRA Fine-Tuned Model ===\n",
        "base = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", device_map=\"auto\", torch_dtype=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistral-lora-reddit\")\n",
        "model = PeftModel.from_pretrained(base, \"mistral-lora-reddit\")\n",
        "\n",
        "# === Setup Pipeline ===\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "\n",
        "# === Recommender Function ===\n",
        "def recommend_posts(user_input, top_k=5):\n",
        "    # Get SBERT embedding of user input\n",
        "    query_embedding = sbert.encode([user_input]).astype(\"float32\")\n",
        "\n",
        "    # Search most similar posts\n",
        "    _, indices = index.search(query_embedding, top_k)\n",
        "    similar_posts = df.iloc[indices[0]][\"clean_body\"].tolist()\n",
        "\n",
        "    # Create prompt\n",
        "    prompt = f\"User post:\\n\\n{user_input}\\n\\nSimilar posts:\\n\"\n",
        "    prompt += \"\\n\".join([f\"{i+1}. {post}\" for i, post in enumerate(similar_posts)])\n",
        "    prompt += \"\\n\\n###\\n\"\n",
        "\n",
        "    # Generate response using Mistral\n",
        "    result = pipe(prompt, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.7)\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "# === Test Example ===\n",
        "user_input = \"Why do cats suddenly run around like crazy at night?\"\n",
        "response = recommend_posts(user_input)\n",
        "print(\"üîç Mistral Recommender Response:\\n\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "ws7xdwNKKGpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing Accuracy of each Model"
      ],
      "metadata": {
        "id": "RIYHyzjVOzqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "id": "OStgUCsQKGwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def evaluate_model(pipe, model_name, test_queries, top_k=5):\n",
        "    total_score = 0.0\n",
        "\n",
        "    for query in test_queries:\n",
        "        # Step 1: Get query embedding\n",
        "        query_embedding = sbert.encode([query]).astype(\"float32\")\n",
        "\n",
        "        # Step 2: Get top-k similar ground truth posts\n",
        "        _, indices = index.search(query_embedding, top_k)\n",
        "        ground_truth_posts = df.iloc[indices[0]][\"clean_body\"].tolist()\n",
        "\n",
        "        # Step 3: Format prompt\n",
        "        prompt = f\"User post:\\n\\n{query}\\n\\nSimilar posts:\\n\"\n",
        "        prompt += \"\\n\".join([f\"{i+1}. {post}\" for i, post in enumerate(ground_truth_posts)])\n",
        "        prompt += \"\\n\\n###\\n\"\n",
        "\n",
        "        # Step 4: Generate output\n",
        "        result = pipe(prompt, max_new_tokens=100, do_sample=True, top_p=0.9, temperature=0.7)\n",
        "        generated = result[0]['generated_text']\n",
        "\n",
        "        # Step 5: Get embedding of generated output\n",
        "        gen_embedding = sbert.encode([generated]).astype(\"float32\")\n",
        "\n",
        "        # Step 6: Compute cosine similarity with top-1 ground truth post\n",
        "        gt_embedding = sbert.encode([ground_truth_posts[0]]).astype(\"float32\")\n",
        "        score = cosine_similarity(gen_embedding, gt_embedding)[0][0]\n",
        "        total_score += score\n",
        "\n",
        "    avg_score = total_score / len(test_queries)\n",
        "    print(f\"üîç {model_name} - Avg Cosine Similarity Accuracy: {avg_score:.4f}\")\n",
        "    return avg_score\n"
      ],
      "metadata": {
        "id": "Gm6JM3rFKGx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_queries = [\n",
        "    \"My cat keeps sitting on my laptop.\",\n",
        "    \"I feel really low after breaking up.\",\n",
        "    \"Why do people ghost others online?\",\n",
        "    \"Any tips to save money in college?\",\n",
        "    \"How to deal with work anxiety?\"\n",
        "]\n",
        "\n",
        "# === DeepSeek Evaluation ===\n",
        "evaluate_model(pipe_deepseek, \"DeepSeek\", test_queries)\n",
        "\n",
        "# === TinyLlama Evaluation ===\n",
        "evaluate_model(pipe_tinyllama, \"TinyLlama\", test_queries)\n",
        "\n",
        "# === Mistral Evaluation ===\n",
        "evaluate_model(pipe_mistral, \"Mistral\", test_queries)\n"
      ],
      "metadata": {
        "id": "S8pSlwgUKG2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o_Drr_n3KG4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-TMK2ofVKG9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XRM6dSdZKG_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WhxIXv7sKHDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7eqneewbKHE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P0GOji9KKHJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UhwDHnKTKHLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ZnRUY5DKHP_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}